{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c7fab7-11a9-43f4-b70d-bd1868e885f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bec2c83-45bd-4615-8e4e-63c4a205060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a52308-522c-413d-b051-a15e65d03c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c1e0796-380a-4e49-a2c8-9739a86316a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de28f286-0dd7-40cb-8c86-29c30e708603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c752b2f9-96a7-4484-920d-45faf2f5519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c67d437-c85a-488c-afbd-43ac4fe5782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e63659e-30f0-4b8a-8717-1a77957e09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aaf3fd2-8bf9-4eca-9c7d-078ba0b743ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM Model\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9c1f8d2-1453-45de-89d5-51881cc38534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c939f98-7f66-4156-aa3b-a2bd725df77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780ebc5b-5ccf-4597-82a1-1fd52c270d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1512f52-7b33-43f4-a491-3ddc7b4abe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697144cd-cd94-4d7b-b247-52f9efe86ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\visca\\anaconda3\\envs\\swahili\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105854 sha256=356148802248e90480764ee99808a69c1d7f4f9804da56e648d0c39e4007babd\n",
      "  Stored in directory: c:\\users\\visca\\appdata\\local\\pip\\cache\\wheels\\ff\\55\\fb\\b27a466be754d2a06ffe0e37b248d844f090a63b51becea85d\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15acd7fa-0d16-4c01-8878-dca679b91541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086ba996-7dca-4aaf-b98b-1920077c5bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    }
   ],
   "source": [
    "!pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd1189c-4d42-412d-9c41-f99715150afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle API Key saved!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "kaggle_json = {\"username\": \"your_username\", \"key\": \"your_api_key\"}\n",
    "\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as file:\n",
    "    json.dump(kaggle_json, file)\n",
    "\n",
    "print(\"Kaggle API Key saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e556035-9fb5-49f1-a567-f3d6041fae69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed1a1c7-e9fa-43fa-9a7c-8e7f3fe84265",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkaggle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Replace 'username/dataset-name' with the actual Kaggle dataset path\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaalbannyantudre/swahili-news-classification-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kaggle'"
     ]
    }
   ],
   "source": [
    "#import kaggle\n",
    "\n",
    "# Replace 'username/dataset-name' with the actual Kaggle dataset path\n",
    "dataset = \"waalbannyantudre/swahili-news-classification-dataset\"\n",
    "\n",
    "# Download the dataset\n",
    "!kaggle datasets download -d {dataset} --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdee5b2d-4174-4d10-8ebf-9a24f20ba28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.6.17.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\visca\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\visca\\anaconda3\\lib\\site-packages (from bleach->kaggle) (24.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\visca\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\visca\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105797 sha256=2155a78bbe212e76125f19c74301d859c8272041fb643de49c6a86fa226a972f\n",
      "  Stored in directory: c:\\users\\visca\\appdata\\local\\pip\\cache\\wheels\\46\\d2\\26\\84d0a1acdb9c6baccf7d28cf06962ec80529fe1ad938489983\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.6.17\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b345a964-7613-4444-939a-42a5034fbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280bc594-937f-48b3-b1be-112c2ac97423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/waalbannyantudre/swahili-news-classification-dataset\n",
      "License(s): Attribution 4.0 International (CC BY 4.0)\n",
      "Downloading swahili-news-classification-dataset.zip to C:\\Users\\Visca\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/18.1M [00:00<?, ?B/s]\n",
      "  6%|5         | 1.00M/18.1M [00:02<00:49, 361kB/s]\n",
      " 11%|#1        | 2.00M/18.1M [00:03<00:23, 715kB/s]\n",
      " 17%|#6        | 3.00M/18.1M [00:03<00:12, 1.23MB/s]\n",
      " 22%|##2       | 4.00M/18.1M [00:03<00:08, 1.67MB/s]\n",
      " 28%|##7       | 5.00M/18.1M [00:04<00:06, 2.14MB/s]\n",
      " 33%|###3      | 6.00M/18.1M [00:04<00:05, 2.35MB/s]\n",
      " 39%|###8      | 7.00M/18.1M [00:05<00:06, 1.73MB/s]\n",
      " 44%|####4     | 8.00M/18.1M [00:06<00:07, 1.49MB/s]\n",
      " 50%|####9     | 9.00M/18.1M [00:07<00:06, 1.41MB/s]\n",
      " 55%|#####5    | 10.0M/18.1M [00:09<00:10, 780kB/s] \n",
      " 61%|######    | 11.0M/18.1M [00:09<00:06, 1.09MB/s]\n",
      " 66%|######6   | 12.0M/18.1M [00:10<00:04, 1.35MB/s]\n",
      " 72%|#######1  | 13.0M/18.1M [00:11<00:04, 1.25MB/s]\n",
      " 77%|#######7  | 14.0M/18.1M [00:12<00:03, 1.24MB/s]\n",
      " 83%|########2 | 15.0M/18.1M [00:12<00:02, 1.25MB/s]\n",
      " 88%|########8 | 16.0M/18.1M [00:13<00:01, 1.19MB/s]\n",
      " 94%|#########3| 17.0M/18.1M [00:14<00:00, 1.28MB/s]\n",
      " 99%|#########9| 18.0M/18.1M [00:15<00:00, 1.23MB/s]\n",
      "100%|##########| 18.1M/18.1M [00:15<00:00, 1.23MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace 'username/dataset-name' with the actual Kaggle dataset path\n",
    "dataset = \"waalbannyantudre/swahili-news-classification-dataset\"\n",
    "\n",
    "# Download the dataset\n",
    "!kaggle datasets download -d {dataset} --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43145712-5b1d-4a8a-855f-ec7004acee39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SW4670</td>\n",
       "      <td>Bodi ya Utalii Tanzania (TTB) imesema, itafan...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SW30826</td>\n",
       "      <td>PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SW29725</td>\n",
       "      <td>Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SW20901</td>\n",
       "      <td>TIMU ya taifa ya Tanzania, Serengeti Boys jan...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SW12560</td>\n",
       "      <td>Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            content category\n",
       "0   SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi\n",
       "1  SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa\n",
       "2  SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi\n",
       "3  SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo\n",
       "4  SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'filename.csv' with the actual CSV file name\n",
    "df = pd.read_csv(\"SwahiliNewsClassificationDataset.csv\")\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23833e0e-2922-4ba4-abba-3894c99597c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "content     0\n",
      "category    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())  # Count missing values\n",
    "df.dropna(inplace=True)   # Remove rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e2daa38-8438-4cf4-8192-c3b5bc3f0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d57a6c2c-ed25-4fae-926c-eaf70c6a26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "442509a9-e5a1-4bdf-81e7-4f39e9b29c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\Visca\\Downloads\\SwahiliNewsClassificationDataset_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60fa1ca9-6c55-4dd1-b76b-5404934cf60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SW4670</td>\n",
       "      <td>Bodi ya Utalii Tanzania (TTB) imesema, itafan...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SW30826</td>\n",
       "      <td>PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SW29725</td>\n",
       "      <td>Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SW20901</td>\n",
       "      <td>TIMU ya taifa ya Tanzania, Serengeti Boys jan...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SW12560</td>\n",
       "      <td>Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            content category\n",
       "0   SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi\n",
       "1  SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa\n",
       "2  SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi\n",
       "3  SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo\n",
       "4  SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a4702db-09c2-48b4-b80b-663eff40b298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23263</th>\n",
       "      <td>SW24920</td>\n",
       "      <td>Alitoa pongezi hizo alipozindua rasmi hatua y...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23264</th>\n",
       "      <td>SW4038</td>\n",
       "      <td>Na NORA DAMIAN-DAR ES SALAAM  TEKLA (si jina ...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23265</th>\n",
       "      <td>SW16649</td>\n",
       "      <td>Mkuu wa Mkoa wa Njombe, Dk Rehema Nchimbi wak...</td>\n",
       "      <td>uchumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23266</th>\n",
       "      <td>SW23291</td>\n",
       "      <td>MABINGWA wa Ligi Kuu Soka Tanzania Bara, Simb...</td>\n",
       "      <td>michezo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23267</th>\n",
       "      <td>SW11778</td>\n",
       "      <td>WIKI iliyopita, nilianza makala haya yanayole...</td>\n",
       "      <td>kitaifa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            content category\n",
       "23263  SW24920   Alitoa pongezi hizo alipozindua rasmi hatua y...   uchumi\n",
       "23264   SW4038   Na NORA DAMIAN-DAR ES SALAAM  TEKLA (si jina ...  kitaifa\n",
       "23265  SW16649   Mkuu wa Mkoa wa Njombe, Dk Rehema Nchimbi wak...   uchumi\n",
       "23266  SW23291   MABINGWA wa Ligi Kuu Soka Tanzania Bara, Simb...  michezo\n",
       "23267  SW11778   WIKI iliyopita, nilianza makala haya yanayole...  kitaifa"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845f4a11-7509-450c-b4d0-1b846ef63827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2512ae39-b2d6-44a9-a642-634ee9346480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2f24b5-034a-45f8-8995-f8b1ee83ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\visca\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Collecting keras\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\visca\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\visca\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\visca\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Collecting absl-py (from keras)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\visca\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\visca\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Collecting namex (from keras)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: h5py in c:\\users\\visca\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Collecting optree (from keras)\n",
      "  Using cached optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\visca\\anaconda3\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\visca\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\visca\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.0/1.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Installing collected packages: namex, optree, ml-dtypes, absl-py, keras\n",
      "Successfully installed absl-py-2.1.0 keras-3.8.0 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753485ce-01ca-4a78-897c-be98a5e557a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867c27f5-1c6e-487e-87bf-2a8ce62b5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88debf3-fd6d-4827-9340-27d2bb38b7ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Combine all text into a single dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_column\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Change \"text_column\" to the actual column name\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize sentences\u001b[39;00m\n\u001b[0;32m     12\u001b[0m sentences \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_column'"
     ]
    }
   ],
   "source": [
    "# Define the Kaggle API file path\n",
    "file_path = \"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\"\n",
    "\n",
    "# Load cleaned Swahili dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine all text into a single dataset\n",
    "text = \" \".join(df[\"text_column\"])  # Change \"text_column\" to the actual column name\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Load cleaned Swahili dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine all text into a single dataset\n",
    "\n",
    "text = \" \".join(df[\"text_column\"])  # Change \"text_column\" to the actual column name\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Load cleaned Swahili dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine all text into a single dataset\n",
    "text = \" \".join(df[\"text_column\"])  # Change \"text_column\" to the actual column name\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Load cleaned Swahili dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine all text into a single dataset\n",
    "text = \" \".join(df[\"text_column\"])  # Change \"text_column\" to the actual column name\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b210db-e38f-4f50-897d-a3fa172d8284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: Index(['id', 'content', 'category'], dtype='object')\n",
      "Using text column: content\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[text_column])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Tokenize sentences\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m sentences \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(text)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"your_dataset.csv\")  # Adjust filename\n",
    "sentences = df[\"text_column\"].tolist()  # Replace 'text_column' with the actual column name\n",
    "print(sentences[:5])  # Check first 5 sentences\n",
    "\n",
    "\n",
    "# Verify if the file exists before reading\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File not found at: {file_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print available columns for debugging\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# Find the correct text column\n",
    "possible_text_columns = [\"text\", \"content\", \"article\", \"body\"]  # Add possible names\n",
    "text_column = next((col for col in possible_text_columns if col in df.columns), None)\n",
    "\n",
    "if text_column is None:\n",
    "    raise KeyError(f\"No valid text column found! Available columns: {df.columns}\")\n",
    "\n",
    "print(f\"Using text column: {text_column}\")\n",
    "\n",
    "# Combine all text into a single dataset\n",
    "text = \" \".join(df[text_column])\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8030a67f-a8b9-407c-a6a7-a383c66e037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = re.split(r'[.!?]', text)  # Splits on '.', '!', or '?'\n",
    "sentences = [s.strip() for s in sentences if s.strip()]  # Clean empty strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c23a13a-11ee-4b0e-b57b-15167c0f020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize and fit tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  # Ensure 'sentences' is already defined\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1  # Define 'total_words' if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4ea8f3-38c6-487f-acf8-c7418338d9cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      2\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m      4\u001b[0m     token_list \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([sentence])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_list)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"pre\")\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90864601-a521-4045-b360-c40bcd3f2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = min(50, max(len(seq) for seq in input_sequences))  # Limit to 50 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d0c1cf9-c865-4319-a6f3-b12d1bbd5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:500000]  # Limit dataset to 500k sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef2c09f-0d11-45e5-8c22-81aa704e7acd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python integer 73932 out of bounds for int16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\keras\\src\\utils\\sequence_utils.py:125\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m trunc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(trunc, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     )\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python integer 73932 out of bounds for int16"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"pre\", dtype=np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a2b5abb-2948-4991-9ad2-f7918bbc925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "input_sequences = []\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch_sentences = sentences[i : i + batch_size]\n",
    "    token_list = tokenizer.texts_to_sequences(batch_sentences)\n",
    "    input_sequences.extend(token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b19ed8-787f-43da-82c0-a8deaac41598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df486aa-4f98-465b-a014-373fd3731a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenize text into sentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized Sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the required NLTK resource (only run once)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Sample text data\n",
    "text = \"Hujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization.\"\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Tokenized Sentences:\", sentences)  # Debugging step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8628e55-38f2-4446-9b88-89835456b486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m----> 4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(sentences)  \u001b[38;5;66;03m# Now `sentences` exists\u001b[39;00m\n\u001b[0;32m      5\u001b[0m total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Get vocabulary size\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  # Now `sentences` exists\n",
    "total_words = len(tokenizer.word_index) + 1  # Get vocabulary size\n",
    "\n",
    "print(f\"Total Words: {total_words}\")  # Debugging output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bbf565c-716a-40b5-8a9e-c3ec42a5dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenize text into sentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized Sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the required NLTK resource (only run once)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Sample text data\n",
    "text = \"Hujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization.\"\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Tokenized Sentences:\", sentences)  # Debugging step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717902a4-03e0-408d-b4df-a1c338ef06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db629246-10c3-4ced-a7c0-2cd07ae5b65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization. \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m          Kiswahili ni lugha nzuri. Tunajifunza Machine Learning kwa Kiswahili.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize text into sentences\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check if sentences exist\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized Sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Download the required NLTK data (only needed once)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Define text data\n",
    "text = \"\"\"Hujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization. \n",
    "          Kiswahili ni lugha nzuri. Tunajifunza Machine Learning kwa Kiswahili.\"\"\"\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Check if sentences exist\n",
    "print(\"Tokenized Sentences:\", sentences)  # Debugging step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76263fb-b4bf-46b8-bb88-dc37008f044a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit tokenizer on sentences\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(sentences)  \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get vocabulary size\u001b[39;00m\n\u001b[0;32m      8\u001b[0m total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on sentences\n",
    "tokenizer.fit_on_texts(sentences)  \n",
    "\n",
    "# Get vocabulary size\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "\n",
    "print(f\"Total Words: {total_words}\")  # Debugging output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d894aa5-06ed-4547-a51e-991f700538ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization. \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m          Kiswahili ni lugha nzuri. Tunajifunza Machine Learning kwa Kiswahili.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize into sentences\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Debugging Step: Print `sentences` before tokenizer\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentences before tokenization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Download punkt if not already downloaded\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Define sample text\n",
    "text = \"\"\"Hujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization. \n",
    "          Kiswahili ni lugha nzuri. Tunajifunza Machine Learning kwa Kiswahili.\"\"\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Debugging Step: Print `sentences` before tokenizer\n",
    "print(\"Sentences before tokenization:\", sentences)\n",
    "\n",
    "# Ensure `sentences` is not empty\n",
    "if not sentences:\n",
    "    raise ValueError(\"Error: `sentences` is empty! Check your text input.\")\n",
    "\n",
    "# Now fit tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  \n",
    "\n",
    "# Get vocabulary size\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "\n",
    "print(f\"Total Words: {total_words}\")  # Debugging output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a09d95-f484-4f5c-b475-790512c2b0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(sent_tokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHii ni mfano wa Kiswahili. Ina sentensi mbili.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(\"Hii ni mfano wa Kiswahili. Ina sentensi mbili.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c06dff-6909-4615-9bb6-838e327cdf0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentences before tokenization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(sentences))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Sentences before tokenization:\", sentences)\n",
    "print(\"Type of sentences:\", type(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa477e39-e740-467a-8812-c8f9d3ead5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Visca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization. \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m          Kiswahili ni lugha nzuri. Tunajifunza Machine Learning kwa Kiswahili.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 3: Tokenize into sentences\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Debugging Check 1: Ensure `sentences` is created\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Sentences created:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Visca/nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\anaconda3\\\\envs\\\\Swahili\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Visca\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Step 1: Download 'punkt' if it's missing\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Step 2: Define a sample text (Use your actual text data here)\n",
    "text = \"\"\"Hujambo! Hii ni sentensi ya Kiswahili. Tunajaribu kufanya tokenization. \n",
    "          Kiswahili ni lugha nzuri. Tunajifunza Machine Learning kwa Kiswahili.\"\"\"\n",
    "\n",
    "# Step 3: Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Debugging Check 1: Ensure `sentences` is created\n",
    "print(\"✅ Sentences created:\", sentences)\n",
    "print(\"✅ Type of sentences:\", type(sentences))\n",
    "if not sentences:\n",
    "    raise ValueError(\"🚨 Error: `sentences` is empty! Check your text input.\")\n",
    "\n",
    "# Step 4: Initialize Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Step 5: Fit tokenizer on sentences\n",
    "try:\n",
    "    tokenizer.fit_on_texts(sentences)  \n",
    "except Exception as e:\n",
    "    print(\"❌ Error while fitting tokenizer:\", e)\n",
    "    raise  # Rethrow the exception for debugging\n",
    "\n",
    "# Step 6: Get vocabulary size\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "\n",
    "# Debugging Check 2: Print total words\n",
    "print(f\"✅ Total Words in Vocabulary: {total_words}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff2f79b-49ea-49b8-9fb8-423b8052c0d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m----> 4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(sentences)  \u001b[38;5;66;03m# Now `sentences` exists\u001b[39;00m\n\u001b[0;32m      5\u001b[0m total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Get vocabulary size\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  # Now `sentences` exists\n",
    "total_words = len(tokenizer.word_index) + 1  # Get vocabulary size\n",
    "\n",
    "print(f\"Total Words: {total_words}\")  # Debugging output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291ec5ae-ec97-47a5-a060-2becb123030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9b3638-2f0f-4b33-84fd-8b1c1e6f525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Visca\\anaconda3\\envs\\Swahili\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Ensure `total_words` and `max_length` are defined before using them\n",
    "total_words = 5000  # Set an appropriate vocabulary size\n",
    "max_length = 100    # Adjust based on your dataset\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_length-1),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(128),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(total_words, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577b6bf9-d9c6-43ab-b522-8c3e2ded7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Ninaenda shule kila siku\",\n",
    "    \"Anapenda kusoma vitabu\",\n",
    "    \"Jana tulitembea mbugani\",\n",
    "    \"Chakula hiki ni kitamu\",\n",
    "    \"Tunajifunza Kiswahili pamoja\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2d327b-38bd-4943-b7f8-1f3f04ff4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ninaenda shule kila siku', 'Anapenda kusoma vitabu', 'Jana tulitembea mbugani', 'Chakula hiki ni kitamu', 'Tunajifunza Kiswahili pamoja']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)  # Debugging check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5064b4c6-b644-41c9-9cc3-388bb4f902d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ninaenda shule kila siku', 'Anapenda kusoma vitabu', 'Jana tulitembea mbugani', 'Chakula hiki ni kitamu', 'Tunajifunza Kiswahili pamoja']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)  # Debugging: Ensure sentences are loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e095f8bd-5be5-4a5c-bc44-668ac6e8fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 18\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  \n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "print(f\"Total Words: {total_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ffbd962-1031-418f-9769-074048b95a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'content', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\")\n",
    "\n",
    "print(df.columns)  # Print available column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1eb646-7587-4aae-af0b-04a89a070059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\")\n",
    "\n",
    "# Extract sentences from the 'content' column\n",
    "sentences = df[\"content\"].tolist()\n",
    "\n",
    "# Print first 5 sentences for debugging\n",
    "print(sentences[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64670895-536b-4f19-b76e-09f6933bbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
    "sentences = df[\"content\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c69d584-d88e-4b05-bbe9-c5f861b3a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[df.columns[0]].tolist()  # Access first column dynamically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccff526a-6aa6-4d71-b8b8-2766e59f8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"content\"].tolist()  # Use \"content\" instead of \"text_column\"\n",
    "print(sentences[:5])  # Check first 5 sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d78515-7ce0-458b-80a4-92b84fc7791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[\"content\"].tolist()  # Extract text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bdedd3-439f-4afd-9bd7-6ade9623749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e47df39-9054-457b-89ad-af4c42b85172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                            content category\n",
      "0   SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi\n",
      "1  SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa\n",
      "2  SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi\n",
      "3  SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo\n",
      "4  SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\")\n",
    "print(df.head())  # Check if data is loaded properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ae7cf0-18c7-4558-99cd-43eaf3af2e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"content\"].dropna().tolist()  # Ensure no missing values\n",
    "print(sentences[:5])  # Print first 5 sentences for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0cedb2-de95-40cb-8f06-7534de39446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'content', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\")\n",
    "\n",
    "# Print column names to confirm\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75afb7eb-b9a2-4961-9bde-3a5f5fcc0a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "# Extract sentences from the 'content' column\n",
    "sentences = df[\"content\"].dropna().tolist()  # Remove NaN values\n",
    "\n",
    "# Print a few samples to confirm\n",
    "print(sentences[:5])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c0dc10-ef92-4c3c-a856-b31958849525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 238469\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize and fit tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index))  # Debugging output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4464a03f-31e3-477d-94b1-14eb62ffedfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                            content category\n",
      "0   SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi\n",
      "1  SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa\n",
      "2  SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi\n",
      "3  SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo\n",
      "4  SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa\n",
      "Index(['id', 'content', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\")\n",
    "\n",
    "# Check if df is loaded properly\n",
    "print(df.head())  # View the first 5 rows\n",
    "print(df.columns)  # See the column names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab28ba66-2647-4428-809b-413a3dc26825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "# Extract sentences from 'content' column, dropping NaN values\n",
    "sentences = df[\"content\"].dropna().tolist()\n",
    "\n",
    "# Print a few sentences to verify\n",
    "print(sentences[:5])  # Output first 5 sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e03794-c411-4d8b-b0f9-587d65a21c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'content' column exists before accessing it\n",
    "if \"content\" in df.columns:\n",
    "    sentences = df[\"content\"].dropna().tolist()\n",
    "    print(sentences[:5])  # Output first 5 sentences\n",
    "else:\n",
    "    print(\"Error: 'content' column not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1945af-830a-492e-a43b-f5905d505165",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m      3\u001b[0m     token_list \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([sentence])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_list)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])\n",
    "\n",
    "print(f\"Generated {len(input_sequences)} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32413b28-6b5c-47bc-9358-f9bae7ea9be2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get max sequence length\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_sequences)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Pad sequences\u001b[39;00m\n\u001b[0;32m      7\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Get max sequence length\n",
    "max_length = max(len(seq) for seq in input_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"pre\")\n",
    "\n",
    "print(f\"Max sequence length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c10698-c1d2-4844-ab08-0b59be251b06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get max sequence length\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_sequences)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Pad sequences\u001b[39;00m\n\u001b[0;32m      7\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Get max sequence length\n",
    "max_length = max(len(seq) for seq in input_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"pre\")\n",
    "\n",
    "print(f\"Max sequence length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d0c39b-141a-4aea-a20b-7bcfcb625254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n",
      "23268\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])  # Check first 5 sentences\n",
    "print(len(sentences))  # Check total number of sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71208c96-27be-41a2-a52f-93b2461cd257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[493, 1, 547, 30, 7174, 958, 3488, 17578, 1, 926, 547, 13, 1371, 999, 27, 518, 76, 1, 340, 560, 53, 340, 623, 14, 40, 17578, 11, 74071, 1371, 1, 4473, 340, 560, 8661, 340, 718, 108000, 340, 656, 2, 58300, 340, 623, 204, 3, 493, 7174, 738, 992, 1696, 6865, 2144, 37, 13, 267, 2, 345, 3, 106, 169, 45, 46, 47, 12962, 795, 136, 37810, 1340, 130, 41, 33, 518, 277, 48844, 763, 3, 395, 9, 203, 1, 547, 27, 15, 58, 738, 6865, 714, 489, 7174, 2553, 582, 13, 1371, 1, 4473, 8661, 37811, 12431, 2, 5990, 6263, 926, 2123, 28, 547, 6370, 11938, 543, 6, 275, 6, 5238, 30, 582, 11, 2643, 2853, 1482, 123, 7, 43, 2, 1340, 41, 1, 1054, 876, 27, 563, 14, 40, 1271, 2123, 28, 547], [7908, 11488, 447, 39, 73, 233, 48845, 3001, 6602, 44, 1274, 114, 3, 218, 49, 13878, 2984, 103, 1, 29080, 3, 1790, 6, 1865, 3, 619, 8, 18, 1, 446, 938, 19, 22620, 1790, 993, 399, 155, 52, 1790, 1865, 320, 49, 324, 12963, 619, 33, 218, 59, 171, 143, 39, 73, 118, 1483, 37, 209, 862, 15955, 773, 3, 109, 1, 7980, 5, 582, 1, 2739, 1, 74, 222, 144, 447, 8, 103, 1, 29080, 3, 1790, 52, 42556, 2, 255, 21, 276, 146, 445, 3, 108001, 2, 42, 173, 19769, 108002, 973, 42557, 103, 2, 42558, 3, 3442, 44, 8, 348, 1, 5631, 4, 103, 11, 439, 104, 1, 892, 4597, 10550, 1865, 64, 4, 5664, 12, 3909, 1485, 3, 304, 22, 186, 4284, 34216, 5466, 892, 1423, 3623, 4, 1865, 37812, 304, 12964, 4, 134, 114, 1274, 892, 114, 3, 218, 108003, 64, 1865, 3, 1722, 810, 3112, 58301, 792, 59, 508, 67, 21, 520, 985, 4, 892, 845, 103, 1, 3829, 320, 2, 254, 248, 19, 108004, 1790, 993], [424, 166, 733, 200, 1, 1003, 1370, 517, 3, 412, 61, 1239, 379, 1, 41, 1, 57, 110, 881, 5, 109, 453, 27, 1113, 854, 2, 200, 11, 2792, 4, 755, 652, 76, 1, 200, 160, 3624, 125, 3402, 27, 93, 1, 501, 855, 14, 40, 4700, 412, 28, 178, 1583, 2, 412, 971, 6603, 4958, 3, 272, 1, 133, 112, 2, 311, 3, 464, 6371, 1, 1003, 6660, 517, 1438, 412, 1239, 379, 1, 41, 1, 57, 110, 881, 4, 260, 1, 452, 1, 76, 2671, 119, 1113, 11, 23, 2750, 517, 3, 1583, 2, 1822, 4, 104, 1, 157, 5, 218, 1, 109, 1, 2471, 144, 6661, 477, 3, 452, 1, 76, 3, 1003, 6531, 3665, 1313, 10, 1113, 11, 1, 41, 1, 57, 110, 881, 854, 2604, 203, 1, 112, 5585, 2, 2365, 4, 260, 608, 1132, 452, 1, 76, 2750, 517, 3, 1583, 2, 1822, 4, 104, 1, 157, 6, 218, 1, 109, 1, 2471, 6662, 5, 157, 1, 675, 1, 1019, 5991, 3665, 8, 62, 52, 1796, 2, 1003, 3865, 634, 1583, 1204, 28, 157, 6, 675, 1822, 2, 412, 780, 13, 203, 1, 133, 232, 8, 210, 9, 1113, 11, 7, 1402, 392, 580, 6, 19, 2440, 64, 13, 203, 61, 29, 8, 1113, 11, 5275, 4, 104, 1, 157, 1855, 231, 5, 260, 1, 733, 229, 2, 1510, 29, 1003, 6472, 1583, 2193, 2, 1822, 993, 1239, 379, 1, 41, 1, 57, 110, 998, 283, 4, 104, 1, 157, 6, 109, 1, 2471, 144, 335, 5, 71, 143, 200, 11, 6663, 284, 12, 129, 109, 1, 3162, 144, 459, 43, 2, 157, 1, 271, 4011, 2482, 3633, 244, 379, 1, 6604, 110, 4, 98, 1, 6423, 4832, 1003, 3249, 92, 517, 16, 123, 7, 280, 3, 6372, 104, 1, 208, 4, 170, 6, 19, 128, 27, 477, 3, 1003, 452, 1, 366, 1624, 5004, 8, 2, 2682, 10, 517, 16, 7, 228, 1, 885, 1, 200, 11, 1512, 228, 1, 652, 4, 165, 20, 611, 1734, 272, 1, 116, 1624, 8, 10, 76, 1, 2482, 37, 284, 12, 129, 109, 6664, 6665, 993, 63, 157, 1, 271, 4011, 701, 3804, 2482, 993], [36, 1, 55, 1, 30, 1523, 2247, 48, 48846, 5, 70, 8899, 74072, 5, 472, 1, 419, 1, 87, 4, 122, 161, 318, 191, 1, 77, 586, 18, 1, 5036, 176, 217, 301, 2, 423, 13, 142, 3, 55, 45, 46, 47, 423, 2948, 234, 1089, 9, 91, 5, 246, 1, 444, 2793, 2, 74073, 2472, 4012, 2783, 3002, 1, 48847, 108005, 63, 2091, 9, 181, 5179, 2, 58302, 7909, 5, 246, 1, 765, 22621, 58303, 1523, 2247, 4381, 2289, 58303, 4242, 423, 234, 9, 168, 2, 9, 224, 2, 1652, 1523, 1494, 4131, 5, 356, 660, 2, 831, 7391, 4, 1847, 3, 235, 27, 1523, 2247, 10902, 60, 3, 285, 101, 1, 1810, 63, 894, 74074, 259, 18, 1, 512, 60, 26, 3, 201, 13, 142, 16, 2, 1458, 247, 302, 18, 1, 512, 1009, 3, 1890, 101, 1, 30], [2, 8065, 1121, 74075, 46, 47, 441, 256, 3, 255, 2, 19, 6, 739, 5, 2843, 1, 115, 12, 2106, 1836, 12165, 18984, 27119, 3064, 3, 115, 99, 2, 2223, 3265, 2, 509, 78, 1, 306, 16, 3, 18984, 37813, 48, 5, 670, 1, 530, 11092, 8662, 230, 63, 37814, 2, 344, 24, 1, 368, 5, 6107, 22, 18984, 2456, 10, 2040, 3, 485, 127, 34217, 2106, 1836, 8, 1996, 1899, 414, 5, 2342, 3, 294, 445, 1, 509, 20, 3266, 95, 41, 3, 702, 296, 192, 6, 37815, 18984, 8, 2207, 20583, 2, 509, 20, 92, 281, 2, 82, 287, 4, 134, 2, 471, 3, 407, 981, 28, 509, 510, 541, 48848, 559, 2, 4, 1213, 2834, 2, 55, 2554, 58304, 42, 58305, 414, 5, 2342, 3, 294, 264, 1, 509, 20, 8663, 95, 1833, 41, 51, 2, 702, 296, 1375, 6, 37815, 108006, 7175, 281, 2, 82, 287, 4, 134, 2, 471, 3, 407, 981, 28, 509, 15, 8, 18984, 8, 50, 2, 306, 16, 49, 1638, 58306, 1156, 74076, 108007, 2, 115, 519, 1021, 7, 679, 26, 822, 17, 146, 841, 5467, 2, 115, 23934, 10, 2040, 3, 115, 12, 509, 7, 213, 397, 1, 3236, 25, 5761, 37816, 4, 31, 316, 211, 58307, 6264, 51, 1928, 58308, 65, 9, 3120, 716, 471, 2640, 3, 2106, 1836, 5, 9244, 638, 1, 271, 1, 3886, 3, 115, 99, 15, 8, 18984, 18984, 8, 3783, 429, 108008, 17, 228, 1, 4930, 90, 1512, 546, 1990, 2, 90, 638, 1, 115, 99, 519, 23935, 17, 2911, 1, 719, 238, 108009, 541, 108010, 74077, 108011, 2, 108012, 20, 74078, 210, 20584, 2, 239, 10, 797, 1858, 1129, 2, 20585, 41, 418, 37817, 2990, 481, 5, 149, 264, 1, 530, 2, 4575, 74079, 2, 18985, 3, 18274, 3, 823, 15, 8, 18984, 18984, 8779, 70, 61, 5, 115, 99, 610, 256, 3, 561, 2, 885, 1579, 3, 215, 131, 1579, 3, 218, 131, 2, 1579, 3, 267, 32, 418, 1055, 6532, 18984, 4, 368, 1952, 6107, 22, 25, 3755, 1517, 6424, 482, 106, 3, 2106, 1836, 1326, 2875, 1639, 6107, 22, 10, 7, 9, 18984, 18984, 2098, 2336, 3, 168, 736, 2106, 1836, 14, 40, 18, 1, 441, 256, 32, 3, 115, 99, 7033, 15026, 2228, 70, 24, 2, 171, 18275, 27, 252, 793, 14, 40, 501, 3554, 469, 714, 14, 40, 2336, 4678, 22622, 230, 2047, 3783, 414, 115, 99, 2, 1402, 392, 795, 2732, 2, 39, 73, 233, 118, 22622, 1280, 10, 265, 3, 5396, 219, 111, 115, 12, 6605, 2184, 127, 1, 14, 48, 475, 1662, 661, 387, 111, 2106, 1836]]\n",
      "23268\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convert sentences to sequences\n",
    "input_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Check if sequences are generated\n",
    "print(input_sequences[:5])  # First 5 tokenized sequences\n",
    "print(len(input_sequences))  # Total sequences generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f844a4f7-a008-42be-b9e0-9e4bb8422236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences Shape: (23268, 3959)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Ensure input_sequences is not empty\n",
    "if input_sequences:\n",
    "    max_length = max(len(seq) for seq in input_sequences)\n",
    "    input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"pre\")\n",
    "    print(f\"Padded Sequences Shape: {input_sequences.shape}\")  # Debugging\n",
    "else:\n",
    "    print(\"Error: input_sequences is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7778d33-cf61-4270-bd95-71a9031fd814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bodi ya Utalii Tanzania (TTB) imesema, itafanya misafara ya kutangaza utalii kwenye miji minne nchini China kati ya Juni 19 hadi Juni 26 mwaka huu.Misafara hiyo itatembelea miji ya Beijing Juni 19, Shanghai Juni 21, Nanjig Juni 24 na Changsha Juni 26.Mwenyekiti wa bodi TTB, Jaji Mstaafu Thomas Mihayo ameyasema hayo kwenye mkutano na waandishi wa habari jijini Dar es Salaam.“Tunafanya jitihada kuhakikisha tunavuna watalii wengi zaidi kutoka China hasa tukizingatia umuhimu wa soko la sekta ya utalii nchini,” amesema Jaji Mihayo.Novemba 2018 TTB ilifanya ziara kwenye miji ya Beijing, Shanghai, Chengdu, Guangzhou na Hong Kong kutangaza vivutio vya utalii sanjari kuzitangaza safari za ndege za Air Tanzania.Ziara hiyo inaelezwa kuzaa matunda ikiwa ni pamoja na watalii zaidi ya 300 kuja nchini Mei mwaka huu kutembelea vivutio vya utalii.', ' PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ametangaza kuwafukuza\\nkazi wakurugenzi wote wa halmashauri ambao watabainika kukiuka sheria ya\\nufutaji wa tozo za ushuru wa mazao. Alisema baada ya kuingia madarakani, Serikali\\niliondoa tozo 80, miongoni mwa hizo tozo ushuru wakulima ambao walikuwa\\nwakilipa mazao kutoka halmashauri moja kwenda nyingine. Rais Dk. Magufuli, aliyasema hayo juzi alipokuwa\\nakiwahutubia wakazi wa Wilaya ya Kyela katika ziara ya kikazi ya siku 10 mkoani\\nMbeya. Alisema, sheria ya ufutaji wa tozo hizo\\nzilipitishwa na Bunge hivyo hakuna mtu yoyote wa kuitengua na kwamba Mkurugenzi\\natakaye itengua atakuwa umevunja sheria na anawajibu wa kufukuzwa kazi. Alisema licha ya kupitishwa kwa sheria hiyo wapo\\nbaadhi ya watendaji wameendelea kuwatoza ushuru wananchi kwa kisingizio cha\\nkupandisha ukusanyaji wa mapato hilo jambo sitaki kulisikia. “Wapo watendaji\\nwanasema kufutwa kwa ushuru kunapunguza mapato, narudia kwa viongozi wote, wakurugenzi,\\nwatendaji wote wa halmashauri msiwatoze wananchi ushuru wa mzigo wowote usio\\nzidi tani moja,”alisema. Hata hivyo, alitoa wito kwa watendaji kuzingatia\\nsheria ya kuwalinda wakulima na ndio maana serikali iliziondoa tozo 80.', 'Mwandishi Wetu -Singida BENKI ya NMB imetoa msaada wa vifaa mbalimbali vyenye thamani ya zaidi ya Sh milioni 200 katika wilaya kadhaa nchini. Misaada iliyotolewa na benki hiyo inayoongoza kwa kutengeneza faida kati ya benki zote zinazofanya biashara humu nchini ndani ya miezi mitano mwaka huu, inahusisha vifaa vya ujenzi, madawati na vifaa vingine vinavyowezesha ukamilishaji wa miradi ya afya,elimu na usalama wa raia. Benki\\xa0 ya NMB imeshatoa\\xa0 msaada wa\\xa0 vifaa vyenye thamani ya zaidi ya Sh milioni 200 kwa mikoa ya Kanda ya Kati. Akifafanua kuhusu misaada hiyo wakati akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule katika Halmashauri ya Wilaya ya Iramba mkoani ingida, Meneja wa Kanda ya Kati wa NMB, Nsolo Mlozi, alibainisha kuwa misaada hiyo ya zaidi ya Sh milioni 200 iliyotolewa imelenga sekta ya elimu, afya\\xa0 na majanga kwa mikoa mitatu ya\\xa0 Kanda ya kati. Akikabidhi msaada wa madawati na vitanda kwa baadhi ya shule za Halmashauri ya Wilaya ya Iramba, MkoaniSingida, katika shule ya Sekondari ya New Kiomboi, Mlozi alisema fedha hizo zilizotolewa na NMB zimetumika kununua madawati, viti vya shule za sekondari,vitanda na vifaa tiba kwenye sekta ya afya. Aidha, alisema lengo la misaada hiyo ni kuunga mkono juhudi za serikali kuwahudumia wananchi kwenye sekta mbalimbali. Pia alisema misaada hiyo imekabidhiwa kwa baadhi ya shule, zahanati, hospitali katika mikoa ya Singida, Dodoma na Manyara. Pia NMB walikabidhi madawati 250 na vitanda 80 vyenye thamani ya zaidi ya Sh milioni 29.5 kwa baadhi ya shule za Wilaya ya Iramba mkoani humo. Katika hatua nyingine, Benki hiyo imevipatia kituo cha Polisi Wilaya ya Kilosa mkoani Morogoro pamoja na shule ya msingi Msowelo mabati 160 yenye thamani ya Sh5 milioni kwa ajili ya kuezekea maboma. NMB iliamua kutoa msaada huo ikiwa ni mpango wa kuzipunguza baadhi ya changamoto kwa taasisi za serikali hapa nchini, Meneja wa NMB kanda ya mashariki, Baraka Ladislaus, alisema na kubainisha kuwa msaada huo ni sehemu ya mikakati ya benki hiyo kurudisha sehemu ya faida kwa jamii ili kusaidia kuinua miradi ya maendeleo. Baraka alisema kuwa kati ya mabati hayo, kituo cha polisi wilaya kimepa tamabati 80 huku shule ya msingi Msowelo nayo ikipata mabati 80.', ' TIMU ya taifa ya Tanzania, Serengeti Boys jana ilijiweka katika nafasi fi nyu katika mashindano ya Mataifa ya Afrika kwa wachezaji wenye umri chini ya miaka 17 baada ya kuchapwa mabao 3-0 na Uganda kwenye Uwanja wa Taifa, Dar es Salaam.Uganda waliandika bao lao la kwanza katika dakika ya 15 lililofungwa na Kawooya Andrew akiunganisha wavuni krosi ya Najibu Viga huku lile la pili likifungwa na Asaba Ivan katika dakika ya 27 Najib Yiga.Serengeti Boys iliendelea kulala, Yiga aliifungia Uganda bao la tatu na la ushindi na kuifanya Serengeti kushika mkia katika Kundi A na kuacha simanzi kwa wapenzi wa soka nchini. Serengeti Boys inasubiri mchezo wa mwisho dhidi ya Senegal huku Nigeria ikisonga mbele baada ya kushinda mchezo wake wa awali kwenye uwanja huo na kufikisha pointi sita baada ya kushinda ule wa ufunguzi dhidi ya Tanzania.', ' Na AGATHA CHARLES –\\xa0DAR ES SALAAM ALIYEKUWA Katibu wa Bunge na Serikali za Mitaa katika Sekretarieti ya Chama cha ACT-Wazalendo, Habibu Mchange, amejiondoa uanachama wa chama hicho na kujiweka kando na siasa. Taarifa ya uamuzi huo wa Mchange ilisambazwa jana katika mitandao ya kijamii, ikionyesha kuandikwa naye huku ikiambatanishwa na namba yake ya simu. Katika andiko hilo, Mchange aliyewahi kuwa mwanachama wa Chadema kabla hajahamia ACT Wazalendo, alisema aliamua kujitoa rasmi katika ushiriki wa aina yoyote ya siasa ili apate muda zaidi wa kusimamia shughuli zake za kijasiriamali. Mchange alisema ameamua kujiengua na siasa ili kutoa fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa. “Kwa moyo mkunjufu kabisa na kwa mapenzi mema na taifa langu, ninathibitisha kwamba nimejitoa rasmi katika ushiriki wa aina yote ya siasa ili nipate muda mwingi zaidi kufanya na kusimamia shughuli zangu za kijasiriamali zinazonitaka nitoe fursa na huduma sawa kwa viongozi na wanachama wa vyama vyote vya siasa,” alisema Mchange. Alisema kutokana na uamuzi huo ambao aliomba uheshimiwe, lolote atakalolifanya lisihusishwe na chama bali iwe ni msimamo wake mwenyewe kama mtu huru asiye na chama. “Ninafahamu kuwa mwanachama wa chama cha siasa ni haki yangu ya kikatiba, lakini nimeamua kuihifadhi kwa sasa mpaka hapo nitakapoona inafaa kufanya vinginevyo. Ninawatakia kila la heri waliokuwa wanachama wenzangu wa ACT Wazalendo katika kuyafikia malengo ya msingi ya uanzishwaji wa chama hicho,” alisema Mchange. Mchange alisema kujiondoa kwake kusihesabiwe kama sehemu ya kukwamisha au kurudisha nyuma matarajio na au malengo ya chama hicho bali ichukuliwe kama chachu ya kufika mbali. “Msivunjike moyo, msirumbane, pendaneni, heshimianeni na shikamaneni ili mfikie lengo. Nitabaki na kuendelea kuwa ndugu yenu, rafiki na swahiba. Zaidi Mtanzania mwenzenu. Tutaendelea kushirikiana katika mambo yote ya kijamii na kimaisha yasiyohusiana na mlengo wa kiitikadi wa kisiasa,” alisema Mchange. Mchange alishika nafasi mbalimbali katika chama hicho ikiwamo Katibu wa Mipango na Mikakati, Mjumbe wa Kamati Kuu, Mjumbe wa Halmashauri Kuu na Mjumbe wa Mkutano Mkuu. MTANZANIA Jumapili lilimtafuta Mchange kwa simu kuthibitisha andiko hilo, lakini hakuweza kupatikana. Alipotafutwa Ofisa Habari wa ACT-Wazalendo, Abdallah Khamis, alikiri andiko hilo kuwa ni la Mchange. Mchange anakuwa mwanasiasa wa tatu kuondoka ACT-Wazalendo mwaka huu baada ya aliyekuwa Katibu Mkuu wa chama hicho, Samson Mwigamba kuachia nafasi yake na kwenda masomoni nchini Kenya, Aprili mwaka huu. Miezi michache baadaye, Novemba mwaka huu mwanasiasa Moses Machali naye alitangaza kujiondoa rasmi chama hicho na kuunga mkono jitihada zinazofanywa na Rais Dk. John Magufuli. Machali aliwahi kuwa Mbunge wa Kasulu Mjini kupitia Chama cha NCCR-Mageuzi kabla ya mwaka jana kushindwa kutetea jimbo lake kupitia ACT-Wazalendo.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset (Ensure the file path is correct)\n",
    "df = pd.read_csv(\"C:/Users/Visca/Downloads/SwahiliNewsClassificationDataset_Cleaned.csv\")\n",
    "\n",
    "# Extract sentences from the 'content' column\n",
    "sentences = df[\"content\"].dropna().tolist()\n",
    "\n",
    "print(sentences[:5])  # Check the first 5 sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99504b0-f2bb-4426-bbc0-bc6dfb0a17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary: 238470\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)  # Fit tokenizer on the dataset\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "\n",
    "print(f\"Total words in vocabulary: {total_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628c9c78-1552-4d92-83f9-f81070b17b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      3\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# List to store sequences\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m      6\u001b[0m     token_list \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([sentence])[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Convert sentence to tokens\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_list)):  \u001b[38;5;66;03m# Generate sub-sequences\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = []  # List to store sequences\n",
    "\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]  # Convert sentence to tokens\n",
    "    for i in range(1, len(token_list)):  # Generate sub-sequences\n",
    "        input_sequences.append(token_list[:i+1])\n",
    "\n",
    "# Check if sequences are created\n",
    "print(f\"Total sequences generated: {len(input_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e337aa33-9459-4cd7-becc-c9fafade293f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# X = all but last word, y = last word\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m input_sequences[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m input_sequences[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# Labels (last word in sequence)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Convert labels to categorical (one-hot encoding)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# X = all but last word, y = last word\n",
    "X = input_sequences[:, :-1]  # Features\n",
    "y = input_sequences[:, -1]   # Labels (last word in sequence)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e4b68d-19f5-4c3e-9c87-a420d45561e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m----> 6\u001b[0m     Embedding(total_words, \u001b[38;5;241m100\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Embedding Layer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     LSTM(\u001b[38;5;241m128\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# First LSTM Layer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     LSTM(\u001b[38;5;241m128\u001b[39m),  \u001b[38;5;66;03m# Second LSTM Layer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Dense Layer\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     Dense(total_words, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Output Layer\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_words' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_length-1),  # Embedding Layer\n",
    "    LSTM(128, return_sequences=True),  # First LSTM Layer\n",
    "    LSTM(128),  # Second LSTM Layer\n",
    "    Dense(128, activation=\"relu\"),  # Dense Layer\n",
    "    Dense(total_words, activation=\"softmax\")  # Output Layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c933d2f2-dfb2-48f6-a684-a6ca87523fa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(input_sequences)  \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Now slicing will work\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m input_sequences[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Features: all words except last\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m input_sequences[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert input_sequences to a NumPy array\n",
    "input_sequences = np.array(input_sequences)  \n",
    "\n",
    "# Now slicing will work\n",
    "X = input_sequences[:, :-1]  # Features: all words except last\n",
    "y = input_sequences[:, -1]   # Labels: last word in sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf0747-9315-4d23-b37a-63f70a5f9e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
